apiVersion: v1
kind: ServiceAccount
metadata:
  name: falconctl-check-sa
  namespace: {{ .Release.Namespace }}

---

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: falconctl-check-role
  namespace: {{ .Release.Namespace }}
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods/exec"]
  verbs: ["create"]

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: falconctl-check-rolebinding
  namespace: {{ .Release.Namespace }}
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: falconctl-check-role
subjects:
- kind: ServiceAccount
  name: falconctl-check-sa
  namespace: {{ .Release.Namespace }}

---
apiVersion: batch/v1
kind: Job
metadata:
  name: falconctl-check
  namespace: {{ .Release.Namespace }}
  annotations:
    "helm.sh/hook": pre-upgrade,pre-delete
    "helm.sh/hook-weight": "-5"
    "helm.sh/hook-delete-policy": "before-hook-creation"
spec:
  template:
    spec:
      serviceAccountName: falconctl-check-sa
      containers:
      - name: falconctl-check
        image: bitnami/kubectl
        command: ["/bin/sh"]
        args:
        - -c
        - |
          if ! kubectl get pods -n {{ .Release.Namespace }} -l app=falcon-sensor -o jsonpath='{.items[*].status.phase}' | grep -q Running; then
            echo "No Falcon pods are running. Proceeding."
            exit 0
          else
            FALCON_POD=$(kubectl get pods -n {{ .Release.Namespace }} -l app=falcon-sensor -o jsonpath='{.items[0].metadata.name}')
            if kubectl exec -n {{ .Release.Namespace }} $FALCON_POD -c falcon-node-sensor -- falconctl -g --protection-status 2>/dev/null | grep -q "Armed=True"; then
              echo "Falcon is armed. Exiting with failure."
              exit 1
            else
              echo "Falcon is not armed. Proceeding."
              exit 0
            fi
          fi
      restartPolicy: Never
  backoffLimit: 0